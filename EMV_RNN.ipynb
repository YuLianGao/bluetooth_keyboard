{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "csv.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whitebird118/bluetooth_keyboard/blob/master/EMV_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DweYe9FcbMK_"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "AVV2e0XKbJeX"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUtoed20cRJJ"
      },
      "source": [
        "# Load CSV data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ap_W4aQcgNT"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/load_data/csv\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/load_data/csv.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/load_data/csv.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/load_data/csv.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-3Xbt0FfGfs"
      },
      "source": [
        "This tutorial provides examples of how to use CSV data with TensorFlow.\n",
        "\n",
        "There are two main parts to this:\n",
        "\n",
        "1. **Loading the data off disk**\n",
        "2. **Pre-processing it into a form suitable for training.**\n",
        "\n",
        "This tutorial focuses on the loading, and gives some quick examples of preprocessing. For a tutorial that focuses on the preprocessing aspect see the [preprocessing layers guide](https://www.tensorflow.org/guide/keras/preprocessing_layers#quick_recipes) and [tutorial](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgZ9gjmPfSnK"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baYFZMW_bJHh"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Make numpy values easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZhJYbJxHNGJ"
      },
      "source": [
        "## In memory data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny5TEgcmHjVx"
      },
      "source": [
        "For any small CSV dataset the simplest way to train a TensorFlow model on it is to load it into memory as a pandas Dataframe or a NumPy array. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgpBOuU8PGFf"
      },
      "source": [
        "A relatively simple example is the [abalone dataset](https://archive.ics.uci.edu/ml/datasets/abalone). \n",
        "\n",
        "* The dataset is small. \n",
        "* All the input features are all limited-range floating point values. \n",
        "\n",
        "Here is how to download the data into a [Pandas `DataFrame`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZVExo9DKoNz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "803f17b4-a7bc-4778-91cd-27aa0847ff36"
      },
      "source": [
        "abalone_train = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/whitebird118/bluetooth_keyboard/master/new.csv\",\n",
        "    names=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\",\"154\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"171\",\"172\",\"173\",\"174\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\",\"201\",\"Label\"])\n",
        "\n",
        "abalone_train.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>...</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "      <th>200</th>\n",
              "      <th>201</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-39.87</td>\n",
              "      <td>-24.92</td>\n",
              "      <td>-28.31</td>\n",
              "      <td>14.09</td>\n",
              "      <td>11.09</td>\n",
              "      <td>7.95</td>\n",
              "      <td>1.64</td>\n",
              "      <td>-6.00</td>\n",
              "      <td>-4.99</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>0.83</td>\n",
              "      <td>-7.31</td>\n",
              "      <td>5.67</td>\n",
              "      <td>1.20</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.56</td>\n",
              "      <td>-2.22</td>\n",
              "      <td>-3.53</td>\n",
              "      <td>-0.68</td>\n",
              "      <td>0.83</td>\n",
              "      <td>-6.83</td>\n",
              "      <td>7.74</td>\n",
              "      <td>1.81</td>\n",
              "      <td>-3.16</td>\n",
              "      <td>-0.68</td>\n",
              "      <td>-4.17</td>\n",
              "      <td>-6.34</td>\n",
              "      <td>-3.86</td>\n",
              "      <td>-1.12</td>\n",
              "      <td>-4.63</td>\n",
              "      <td>-0.19</td>\n",
              "      <td>0.59</td>\n",
              "      <td>-1.94</td>\n",
              "      <td>0.05</td>\n",
              "      <td>3.03</td>\n",
              "      <td>-0.60</td>\n",
              "      <td>-3.12</td>\n",
              "      <td>30.01</td>\n",
              "      <td>36.51</td>\n",
              "      <td>8.72</td>\n",
              "      <td>...</td>\n",
              "      <td>2.49</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>-3.53</td>\n",
              "      <td>-1.05</td>\n",
              "      <td>-2.09</td>\n",
              "      <td>-2.55</td>\n",
              "      <td>-0.32</td>\n",
              "      <td>0.59</td>\n",
              "      <td>-2.80</td>\n",
              "      <td>-2.02</td>\n",
              "      <td>1.69</td>\n",
              "      <td>-3.77</td>\n",
              "      <td>0.54</td>\n",
              "      <td>4.13</td>\n",
              "      <td>-4.02</td>\n",
              "      <td>-1.41</td>\n",
              "      <td>0.59</td>\n",
              "      <td>-3.29</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.69</td>\n",
              "      <td>-1.33</td>\n",
              "      <td>0.66</td>\n",
              "      <td>2.30</td>\n",
              "      <td>-7.92</td>\n",
              "      <td>-1.66</td>\n",
              "      <td>-2.71</td>\n",
              "      <td>-8.53</td>\n",
              "      <td>1.39</td>\n",
              "      <td>1.93</td>\n",
              "      <td>-1.94</td>\n",
              "      <td>2.25</td>\n",
              "      <td>2.79</td>\n",
              "      <td>-1.09</td>\n",
              "      <td>-0.32</td>\n",
              "      <td>2.18</td>\n",
              "      <td>-4.14</td>\n",
              "      <td>1.76</td>\n",
              "      <td>0.96</td>\n",
              "      <td>-5.97</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9.57</td>\n",
              "      <td>-1.73</td>\n",
              "      <td>-26.23</td>\n",
              "      <td>-28.03</td>\n",
              "      <td>-23.70</td>\n",
              "      <td>-21.47</td>\n",
              "      <td>37.04</td>\n",
              "      <td>30.38</td>\n",
              "      <td>23.94</td>\n",
              "      <td>-0.68</td>\n",
              "      <td>3.28</td>\n",
              "      <td>-3.90</td>\n",
              "      <td>-5.56</td>\n",
              "      <td>-4.29</td>\n",
              "      <td>-10.37</td>\n",
              "      <td>3.71</td>\n",
              "      <td>6.21</td>\n",
              "      <td>-2.55</td>\n",
              "      <td>3.59</td>\n",
              "      <td>1.20</td>\n",
              "      <td>-1.45</td>\n",
              "      <td>-0.44</td>\n",
              "      <td>0.35</td>\n",
              "      <td>-5.24</td>\n",
              "      <td>-0.93</td>\n",
              "      <td>-3.68</td>\n",
              "      <td>-4.63</td>\n",
              "      <td>0.78</td>\n",
              "      <td>3.15</td>\n",
              "      <td>-4.14</td>\n",
              "      <td>1.64</td>\n",
              "      <td>3.03</td>\n",
              "      <td>-2.92</td>\n",
              "      <td>-1.29</td>\n",
              "      <td>-0.39</td>\n",
              "      <td>-4.87</td>\n",
              "      <td>-0.93</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>-1.94</td>\n",
              "      <td>-1.05</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.15</td>\n",
              "      <td>-0.26</td>\n",
              "      <td>-4.99</td>\n",
              "      <td>3.47</td>\n",
              "      <td>2.18</td>\n",
              "      <td>-1.82</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.96</td>\n",
              "      <td>-2.31</td>\n",
              "      <td>1.88</td>\n",
              "      <td>0.10</td>\n",
              "      <td>-4.51</td>\n",
              "      <td>-1.41</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>-4.87</td>\n",
              "      <td>-1.41</td>\n",
              "      <td>-1.85</td>\n",
              "      <td>-7.68</td>\n",
              "      <td>-3.73</td>\n",
              "      <td>-1.36</td>\n",
              "      <td>-7.19</td>\n",
              "      <td>-1.17</td>\n",
              "      <td>1.32</td>\n",
              "      <td>-3.77</td>\n",
              "      <td>1.27</td>\n",
              "      <td>0.83</td>\n",
              "      <td>-4.38</td>\n",
              "      <td>-0.44</td>\n",
              "      <td>0.35</td>\n",
              "      <td>-3.90</td>\n",
              "      <td>2.49</td>\n",
              "      <td>2.42</td>\n",
              "      <td>-2.67</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.10</td>\n",
              "      <td>-1.58</td>\n",
              "      <td>2.13</td>\n",
              "      <td>1.57</td>\n",
              "      <td>-3.16</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12.62</td>\n",
              "      <td>-2.09</td>\n",
              "      <td>15.15</td>\n",
              "      <td>-7.76</td>\n",
              "      <td>4.74</td>\n",
              "      <td>-4.51</td>\n",
              "      <td>2.61</td>\n",
              "      <td>-6.98</td>\n",
              "      <td>-14.15</td>\n",
              "      <td>0.17</td>\n",
              "      <td>2.79</td>\n",
              "      <td>3.43</td>\n",
              "      <td>-1.41</td>\n",
              "      <td>-0.87</td>\n",
              "      <td>-6.70</td>\n",
              "      <td>1.15</td>\n",
              "      <td>0.35</td>\n",
              "      <td>-4.51</td>\n",
              "      <td>3.84</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-3.41</td>\n",
              "      <td>1.88</td>\n",
              "      <td>2.06</td>\n",
              "      <td>-5.85</td>\n",
              "      <td>-0.44</td>\n",
              "      <td>1.20</td>\n",
              "      <td>-4.02</td>\n",
              "      <td>-2.02</td>\n",
              "      <td>-0.26</td>\n",
              "      <td>-0.60</td>\n",
              "      <td>-1.17</td>\n",
              "      <td>2.67</td>\n",
              "      <td>-3.90</td>\n",
              "      <td>-0.19</td>\n",
              "      <td>0.71</td>\n",
              "      <td>-7.68</td>\n",
              "      <td>-2.15</td>\n",
              "      <td>1.45</td>\n",
              "      <td>-4.87</td>\n",
              "      <td>4.32</td>\n",
              "      <td>...</td>\n",
              "      <td>1.39</td>\n",
              "      <td>-1.85</td>\n",
              "      <td>-7.31</td>\n",
              "      <td>0.66</td>\n",
              "      <td>2.54</td>\n",
              "      <td>-6.21</td>\n",
              "      <td>-2.88</td>\n",
              "      <td>-1.61</td>\n",
              "      <td>-4.63</td>\n",
              "      <td>1.39</td>\n",
              "      <td>2.91</td>\n",
              "      <td>-2.92</td>\n",
              "      <td>-3.12</td>\n",
              "      <td>2.42</td>\n",
              "      <td>-7.92</td>\n",
              "      <td>2.98</td>\n",
              "      <td>0.96</td>\n",
              "      <td>-1.45</td>\n",
              "      <td>2.37</td>\n",
              "      <td>2.54</td>\n",
              "      <td>-1.09</td>\n",
              "      <td>-2.02</td>\n",
              "      <td>0.96</td>\n",
              "      <td>-6.34</td>\n",
              "      <td>-1.78</td>\n",
              "      <td>1.08</td>\n",
              "      <td>-5.73</td>\n",
              "      <td>3.35</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.38</td>\n",
              "      <td>-1.41</td>\n",
              "      <td>-0.39</td>\n",
              "      <td>-2.55</td>\n",
              "      <td>-0.56</td>\n",
              "      <td>-0.26</td>\n",
              "      <td>-5.73</td>\n",
              "      <td>-0.80</td>\n",
              "      <td>-0.39</td>\n",
              "      <td>-6.46</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25.93</td>\n",
              "      <td>-14.91</td>\n",
              "      <td>-18.30</td>\n",
              "      <td>2.00</td>\n",
              "      <td>12.92</td>\n",
              "      <td>2.94</td>\n",
              "      <td>22.63</td>\n",
              "      <td>14.63</td>\n",
              "      <td>3.18</td>\n",
              "      <td>-1.78</td>\n",
              "      <td>8.65</td>\n",
              "      <td>-6.34</td>\n",
              "      <td>-1.54</td>\n",
              "      <td>-1.24</td>\n",
              "      <td>-7.31</td>\n",
              "      <td>-0.32</td>\n",
              "      <td>1.81</td>\n",
              "      <td>-2.55</td>\n",
              "      <td>-0.68</td>\n",
              "      <td>0.47</td>\n",
              "      <td>-6.95</td>\n",
              "      <td>4.32</td>\n",
              "      <td>0.22</td>\n",
              "      <td>-1.45</td>\n",
              "      <td>4.20</td>\n",
              "      <td>2.06</td>\n",
              "      <td>-3.16</td>\n",
              "      <td>4.20</td>\n",
              "      <td>0.47</td>\n",
              "      <td>-2.55</td>\n",
              "      <td>3.22</td>\n",
              "      <td>2.91</td>\n",
              "      <td>-1.21</td>\n",
              "      <td>-0.56</td>\n",
              "      <td>2.06</td>\n",
              "      <td>-4.02</td>\n",
              "      <td>2.37</td>\n",
              "      <td>1.81</td>\n",
              "      <td>-0.72</td>\n",
              "      <td>6.15</td>\n",
              "      <td>...</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.93</td>\n",
              "      <td>-3.65</td>\n",
              "      <td>2.13</td>\n",
              "      <td>0.10</td>\n",
              "      <td>-2.19</td>\n",
              "      <td>-0.68</td>\n",
              "      <td>3.64</td>\n",
              "      <td>-3.77</td>\n",
              "      <td>-2.15</td>\n",
              "      <td>5.96</td>\n",
              "      <td>-3.41</td>\n",
              "      <td>-1.17</td>\n",
              "      <td>2.30</td>\n",
              "      <td>-5.85</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.35</td>\n",
              "      <td>-2.92</td>\n",
              "      <td>-2.88</td>\n",
              "      <td>0.47</td>\n",
              "      <td>-5.48</td>\n",
              "      <td>-1.05</td>\n",
              "      <td>3.03</td>\n",
              "      <td>-1.94</td>\n",
              "      <td>0.66</td>\n",
              "      <td>2.91</td>\n",
              "      <td>-5.97</td>\n",
              "      <td>1.39</td>\n",
              "      <td>2.91</td>\n",
              "      <td>-1.09</td>\n",
              "      <td>-1.41</td>\n",
              "      <td>1.08</td>\n",
              "      <td>-7.80</td>\n",
              "      <td>2.98</td>\n",
              "      <td>3.89</td>\n",
              "      <td>-7.68</td>\n",
              "      <td>0.42</td>\n",
              "      <td>-1.97</td>\n",
              "      <td>-7.80</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-30.47</td>\n",
              "      <td>27.69</td>\n",
              "      <td>-29.16</td>\n",
              "      <td>10.79</td>\n",
              "      <td>8.53</td>\n",
              "      <td>4.89</td>\n",
              "      <td>13.48</td>\n",
              "      <td>16.95</td>\n",
              "      <td>15.76</td>\n",
              "      <td>-0.56</td>\n",
              "      <td>-6.12</td>\n",
              "      <td>-9.88</td>\n",
              "      <td>-3.98</td>\n",
              "      <td>2.06</td>\n",
              "      <td>-6.09</td>\n",
              "      <td>-0.80</td>\n",
              "      <td>0.71</td>\n",
              "      <td>-2.92</td>\n",
              "      <td>-2.63</td>\n",
              "      <td>3.89</td>\n",
              "      <td>-1.94</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>1.69</td>\n",
              "      <td>-4.26</td>\n",
              "      <td>-3.98</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-3.53</td>\n",
              "      <td>-3.86</td>\n",
              "      <td>-2.09</td>\n",
              "      <td>-6.34</td>\n",
              "      <td>-1.90</td>\n",
              "      <td>-1.12</td>\n",
              "      <td>-3.29</td>\n",
              "      <td>-0.80</td>\n",
              "      <td>-1.85</td>\n",
              "      <td>-3.41</td>\n",
              "      <td>1.52</td>\n",
              "      <td>-1.36</td>\n",
              "      <td>1.84</td>\n",
              "      <td>-56.83</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.17</td>\n",
              "      <td>3.52</td>\n",
              "      <td>-5.60</td>\n",
              "      <td>-2.88</td>\n",
              "      <td>-0.14</td>\n",
              "      <td>-3.16</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>1.69</td>\n",
              "      <td>-0.23</td>\n",
              "      <td>-0.68</td>\n",
              "      <td>-3.32</td>\n",
              "      <td>-6.83</td>\n",
              "      <td>0.42</td>\n",
              "      <td>-0.51</td>\n",
              "      <td>-7.44</td>\n",
              "      <td>4.57</td>\n",
              "      <td>2.42</td>\n",
              "      <td>-3.41</td>\n",
              "      <td>3.35</td>\n",
              "      <td>4.37</td>\n",
              "      <td>-1.45</td>\n",
              "      <td>1.15</td>\n",
              "      <td>2.67</td>\n",
              "      <td>-2.55</td>\n",
              "      <td>-4.47</td>\n",
              "      <td>-2.58</td>\n",
              "      <td>-9.02</td>\n",
              "      <td>-0.19</td>\n",
              "      <td>-0.51</td>\n",
              "      <td>-3.90</td>\n",
              "      <td>-0.32</td>\n",
              "      <td>-2.83</td>\n",
              "      <td>-3.41</td>\n",
              "      <td>-1.29</td>\n",
              "      <td>2.18</td>\n",
              "      <td>-1.45</td>\n",
              "      <td>0.91</td>\n",
              "      <td>1.93</td>\n",
              "      <td>-5.36</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 202 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       1      2      3      4      5  ...   198   199   200   201  Label\n",
              "0 -39.87 -24.92 -28.31  14.09  11.09  ... -4.14  1.76  0.96 -5.97      1\n",
              "1   9.57  -1.73 -26.23 -28.03 -23.70  ... -1.58  2.13  1.57 -3.16      1\n",
              "2  12.62  -2.09  15.15  -7.76   4.74  ... -5.73 -0.80 -0.39 -6.46      1\n",
              "3  25.93 -14.91 -18.30   2.00  12.92  ... -7.68  0.42 -1.97 -7.80      1\n",
              "4 -30.47  27.69 -29.16  10.79   8.53  ... -1.45  0.91  1.93 -5.36      1\n",
              "\n",
              "[5 rows x 202 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlfGrk_9N-wf"
      },
      "source": [
        "The nominal task for this dataset is to predict the age from the other measurements, so separate the features and labels for training:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udOnDJOxNi7p"
      },
      "source": [
        "abalone_features = abalone_train.copy()\n",
        "abalone_labels = abalone_features.pop('Label')\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seK9n71-UBfT"
      },
      "source": [
        "For this dataset you will treat all features identically. Pack the features into a single NumPy array.:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp3N5McbUMwb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9517628-af97-44a4-d7da-e5e8300547fb"
      },
      "source": [
        "abalone_features = np.array(abalone_features)\n",
        "\n",
        "abalone_features\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-39.87, -24.92, -28.31, ...,   1.76,   0.96,  -5.97],\n",
              "       [  9.57,  -1.73, -26.23, ...,   2.13,   1.57,  -3.16],\n",
              "       [ 12.62,  -2.09,  15.15, ...,  -0.8 ,  -0.39,  -6.46],\n",
              "       ...,\n",
              "       [  6.51,  12.89,  73.97, ...,   0.16,  -1.27,  -6.6 ],\n",
              "       [ -9.73, -14.45, -47.61, ...,   3.33,   1.3 ,  -0.01],\n",
              "       [ -0.94,  17.04,  -7.82, ...,   0.28,  -1.02,  -3.3 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C1yFOxLOdxh"
      },
      "source": [
        "Next make a regression model predict the age. Since there is only a single input tensor, a `keras.Sequential` model is sufficient here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8zzNrZqOmfB"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(input_dim=201, output_dim=3),\n",
        " \n",
        "])\n",
        "\n",
        "model.add(layers.RNN(tf.keras.layers.LSTMCell(64), input_shape=(None, 201)))\n",
        "\n",
        "model.add(layers.Dense(3))\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(abalone_features, abalone_labels, train_size=0.8, test_size=0.2)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6IWeP78O2wE"
      },
      "source": [
        "To train that model, pass the features and labels to `Model.fit`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZdpCD92SN3Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "0d5eabb0-da80-4f59-9289-64c68e25a69c"
      },
      "source": [
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=\"sgd\",\n",
        "    metrics=['acc'])\n",
        "\n",
        "model.fit(\n",
        "    x_train, y_train, validation_data=(x_test, y_test), batch_size=64, epochs=10\n",
        ")\n",
        "\n",
        "\n",
        "loss1, acc1, mse1 = model.evaluate(x_test, y_test)\n",
        "\n",
        "print(f\"Loss is {loss1},\\nAccuracy is {acc1*100},\\nMSE is {mse1}\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-fce3c8b01c16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m model.fit(\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  indices[0,0] = -14 is not in [0, 201)\n\t [[node sequential_22/embedding_21/embedding_lookup (defined at <ipython-input-67-fce3c8b01c16>:6) ]] [Op:__inference_train_function_105978]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential_22/embedding_21/embedding_lookup:\n sequential_22/embedding_21/embedding_lookup/105210 (defined at /usr/lib/python3.7/contextlib.py:112)\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GapLOj1OOTQH"
      },
      "source": [
        "You have just seen the most basic way to train a model using CSV data. Next, you will learn how to apply preprocessing to normalize numeric columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B87Rd1SOUv02"
      },
      "source": [
        "## Basic preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCrB2Jd-U0Vt"
      },
      "source": [
        "It's good practice to normalize the inputs to your model. The `experimental.preprocessing` layers provide a convenient way to build this normalization into your model. \n",
        "\n",
        "The layer will precompute the mean and variance of each column, and use these to normalize the data.\n",
        "\n",
        "First you create the layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2WQpDU5VRk7"
      },
      "source": [
        "normalize = preprocessing.Normalization()"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGgEZE-7Vpt6"
      },
      "source": [
        "Then you use the `Normalization.adapt()` method to adapt the normalization layer to your data.\n",
        "\n",
        "Note: Only use your training data to `.adapt()` preprocessing layers. Do not use your validation or test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WgOPIiOVpLg"
      },
      "source": [
        "normalize.adapt(abalone_features)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE6vh0byV7cE"
      },
      "source": [
        "Then use the normalization layer in your model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quPcZ9dTWA9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73a03de7-574c-42a3-a7b4-739f5e6d6a45"
      },
      "source": [
        "norm_abalone_model = tf.keras.Sequential([\n",
        "  normalize,\n",
        "  layers.Dense(67, activation='relu'),\n",
        "  layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "norm_abalone_model.compile(loss = tf.losses.MeanSquaredError(),\n",
        "                           optimizer = tf.optimizers.Adam(),\n",
        "                           metrics=['accuracy'])\n",
        "\n",
        "norm_abalone_model.fit(abalone_features, abalone_labels, epochs=100)\n"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0196\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbe24a66050>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    }
  ]
}