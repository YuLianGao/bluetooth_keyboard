{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "csv.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whitebird118/bluetooth_keyboard/blob/master/EMV_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DweYe9FcbMK_"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "AVV2e0XKbJeX"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUtoed20cRJJ"
      },
      "source": [
        "# Load CSV data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ap_W4aQcgNT"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/load_data/csv\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/load_data/csv.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/load_data/csv.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/load_data/csv.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-3Xbt0FfGfs"
      },
      "source": [
        "This tutorial provides examples of how to use CSV data with TensorFlow.\n",
        "\n",
        "There are two main parts to this:\n",
        "\n",
        "1. **Loading the data off disk**\n",
        "2. **Pre-processing it into a form suitable for training.**\n",
        "\n",
        "This tutorial focuses on the loading, and gives some quick examples of preprocessing. For a tutorial that focuses on the preprocessing aspect see the [preprocessing layers guide](https://www.tensorflow.org/guide/keras/preprocessing_layers#quick_recipes) and [tutorial](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgZ9gjmPfSnK"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baYFZMW_bJHh"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Make numpy values easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ],
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZhJYbJxHNGJ"
      },
      "source": [
        "## In memory data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny5TEgcmHjVx"
      },
      "source": [
        "For any small CSV dataset the simplest way to train a TensorFlow model on it is to load it into memory as a pandas Dataframe or a NumPy array. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgpBOuU8PGFf"
      },
      "source": [
        "A relatively simple example is the [abalone dataset](https://archive.ics.uci.edu/ml/datasets/abalone). \n",
        "\n",
        "* The dataset is small. \n",
        "* All the input features are all limited-range floating point values. \n",
        "\n",
        "Here is how to download the data into a [Pandas `DataFrame`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZVExo9DKoNz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "40e9272c-f541-441f-a950-d57359493fad"
      },
      "source": [
        "abalone_train = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/whitebird118/bluetooth_keyboard/master/entire.csv\",\n",
        "    names=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\",\"154\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"171\",\"172\",\"173\",\"174\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\",\"201\",\"Label\"])\n",
        "\n",
        "abalone_train.head()"
      ],
      "execution_count": 411,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>...</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "      <th>200</th>\n",
              "      <th>201</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.93</td>\n",
              "      <td>0.60</td>\n",
              "      <td>10.00</td>\n",
              "      <td>4.93</td>\n",
              "      <td>5.24</td>\n",
              "      <td>9.88</td>\n",
              "      <td>3.25</td>\n",
              "      <td>1.47</td>\n",
              "      <td>0.73</td>\n",
              "      <td>3.61</td>\n",
              "      <td>4.28</td>\n",
              "      <td>2.19</td>\n",
              "      <td>3.59</td>\n",
              "      <td>4.02</td>\n",
              "      <td>6.10</td>\n",
              "      <td>0.17</td>\n",
              "      <td>1.70</td>\n",
              "      <td>9.15</td>\n",
              "      <td>2.86</td>\n",
              "      <td>3.65</td>\n",
              "      <td>4.63</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.36</td>\n",
              "      <td>3.78</td>\n",
              "      <td>0.41</td>\n",
              "      <td>3.41</td>\n",
              "      <td>3.41</td>\n",
              "      <td>2.03</td>\n",
              "      <td>1.23</td>\n",
              "      <td>4.75</td>\n",
              "      <td>0.20</td>\n",
              "      <td>1.33</td>\n",
              "      <td>3.53</td>\n",
              "      <td>1.15</td>\n",
              "      <td>1.70</td>\n",
              "      <td>4.63</td>\n",
              "      <td>4.47</td>\n",
              "      <td>0.99</td>\n",
              "      <td>4.51</td>\n",
              "      <td>3.37</td>\n",
              "      <td>...</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.13</td>\n",
              "      <td>8.29</td>\n",
              "      <td>1.15</td>\n",
              "      <td>3.77</td>\n",
              "      <td>5.49</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.48</td>\n",
              "      <td>7.19</td>\n",
              "      <td>1.05</td>\n",
              "      <td>2.43</td>\n",
              "      <td>8.29</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.97</td>\n",
              "      <td>1.27</td>\n",
              "      <td>1.35</td>\n",
              "      <td>7.19</td>\n",
              "      <td>1.29</td>\n",
              "      <td>3.79</td>\n",
              "      <td>0.24</td>\n",
              "      <td>1.54</td>\n",
              "      <td>0.72</td>\n",
              "      <td>1.46</td>\n",
              "      <td>1.78</td>\n",
              "      <td>0.99</td>\n",
              "      <td>4.14</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.48</td>\n",
              "      <td>2.92</td>\n",
              "      <td>1.17</td>\n",
              "      <td>0.11</td>\n",
              "      <td>1.70</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2.45</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.72</td>\n",
              "      <td>4.63</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18.97</td>\n",
              "      <td>14.29</td>\n",
              "      <td>4.77</td>\n",
              "      <td>21.07</td>\n",
              "      <td>10.63</td>\n",
              "      <td>7.70</td>\n",
              "      <td>4.71</td>\n",
              "      <td>2.94</td>\n",
              "      <td>5.49</td>\n",
              "      <td>1.29</td>\n",
              "      <td>0.01</td>\n",
              "      <td>6.95</td>\n",
              "      <td>5.20</td>\n",
              "      <td>2.94</td>\n",
              "      <td>3.90</td>\n",
              "      <td>0.44</td>\n",
              "      <td>2.92</td>\n",
              "      <td>2.19</td>\n",
              "      <td>1.66</td>\n",
              "      <td>1.70</td>\n",
              "      <td>4.02</td>\n",
              "      <td>2.64</td>\n",
              "      <td>3.04</td>\n",
              "      <td>1.70</td>\n",
              "      <td>1.05</td>\n",
              "      <td>1.23</td>\n",
              "      <td>1.35</td>\n",
              "      <td>3.74</td>\n",
              "      <td>2.94</td>\n",
              "      <td>2.07</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.23</td>\n",
              "      <td>7.19</td>\n",
              "      <td>1.76</td>\n",
              "      <td>0.38</td>\n",
              "      <td>5.61</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.36</td>\n",
              "      <td>7.44</td>\n",
              "      <td>4.47</td>\n",
              "      <td>...</td>\n",
              "      <td>1.51</td>\n",
              "      <td>1.33</td>\n",
              "      <td>6.46</td>\n",
              "      <td>1.29</td>\n",
              "      <td>2.45</td>\n",
              "      <td>4.27</td>\n",
              "      <td>3.49</td>\n",
              "      <td>2.19</td>\n",
              "      <td>7.93</td>\n",
              "      <td>2.88</td>\n",
              "      <td>3.43</td>\n",
              "      <td>3.41</td>\n",
              "      <td>0.93</td>\n",
              "      <td>1.09</td>\n",
              "      <td>5.97</td>\n",
              "      <td>1.39</td>\n",
              "      <td>0.48</td>\n",
              "      <td>4.39</td>\n",
              "      <td>1.91</td>\n",
              "      <td>0.13</td>\n",
              "      <td>2.56</td>\n",
              "      <td>1.15</td>\n",
              "      <td>0.36</td>\n",
              "      <td>4.51</td>\n",
              "      <td>1.88</td>\n",
              "      <td>2.94</td>\n",
              "      <td>3.53</td>\n",
              "      <td>3.13</td>\n",
              "      <td>0.60</td>\n",
              "      <td>4.75</td>\n",
              "      <td>3.00</td>\n",
              "      <td>0.60</td>\n",
              "      <td>4.27</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.31</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.78</td>\n",
              "      <td>1.60</td>\n",
              "      <td>3.90</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.23</td>\n",
              "      <td>12.20</td>\n",
              "      <td>15.14</td>\n",
              "      <td>3.47</td>\n",
              "      <td>7.68</td>\n",
              "      <td>15.25</td>\n",
              "      <td>3.34</td>\n",
              "      <td>0.72</td>\n",
              "      <td>3.90</td>\n",
              "      <td>4.35</td>\n",
              "      <td>2.57</td>\n",
              "      <td>1.11</td>\n",
              "      <td>1.66</td>\n",
              "      <td>0.50</td>\n",
              "      <td>5.36</td>\n",
              "      <td>1.02</td>\n",
              "      <td>0.84</td>\n",
              "      <td>7.68</td>\n",
              "      <td>3.37</td>\n",
              "      <td>0.97</td>\n",
              "      <td>11.22</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.60</td>\n",
              "      <td>3.53</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.82</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.13</td>\n",
              "      <td>5.36</td>\n",
              "      <td>2.88</td>\n",
              "      <td>0.23</td>\n",
              "      <td>2.19</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.82</td>\n",
              "      <td>0.36</td>\n",
              "      <td>4.71</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.62</td>\n",
              "      <td>7.76</td>\n",
              "      <td>...</td>\n",
              "      <td>2.64</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.95</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2.07</td>\n",
              "      <td>4.88</td>\n",
              "      <td>3.71</td>\n",
              "      <td>2.31</td>\n",
              "      <td>1.34</td>\n",
              "      <td>0.44</td>\n",
              "      <td>2.94</td>\n",
              "      <td>1.21</td>\n",
              "      <td>4.59</td>\n",
              "      <td>2.57</td>\n",
              "      <td>0.24</td>\n",
              "      <td>1.63</td>\n",
              "      <td>2.43</td>\n",
              "      <td>7.32</td>\n",
              "      <td>1.63</td>\n",
              "      <td>1.84</td>\n",
              "      <td>2.19</td>\n",
              "      <td>0.93</td>\n",
              "      <td>4.14</td>\n",
              "      <td>7.32</td>\n",
              "      <td>0.29</td>\n",
              "      <td>3.53</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.60</td>\n",
              "      <td>8.29</td>\n",
              "      <td>2.88</td>\n",
              "      <td>0.36</td>\n",
              "      <td>3.29</td>\n",
              "      <td>3.25</td>\n",
              "      <td>0.60</td>\n",
              "      <td>3.65</td>\n",
              "      <td>0.41</td>\n",
              "      <td>3.77</td>\n",
              "      <td>6.46</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9.69</td>\n",
              "      <td>8.17</td>\n",
              "      <td>11.10</td>\n",
              "      <td>13.62</td>\n",
              "      <td>8.55</td>\n",
              "      <td>11.96</td>\n",
              "      <td>9.69</td>\n",
              "      <td>2.55</td>\n",
              "      <td>11.47</td>\n",
              "      <td>0.29</td>\n",
              "      <td>1.84</td>\n",
              "      <td>6.83</td>\n",
              "      <td>5.57</td>\n",
              "      <td>1.84</td>\n",
              "      <td>4.63</td>\n",
              "      <td>0.44</td>\n",
              "      <td>1.35</td>\n",
              "      <td>5.24</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.60</td>\n",
              "      <td>6.71</td>\n",
              "      <td>1.05</td>\n",
              "      <td>2.07</td>\n",
              "      <td>6.22</td>\n",
              "      <td>1.27</td>\n",
              "      <td>0.01</td>\n",
              "      <td>4.88</td>\n",
              "      <td>1.15</td>\n",
              "      <td>0.74</td>\n",
              "      <td>8.66</td>\n",
              "      <td>2.64</td>\n",
              "      <td>1.82</td>\n",
              "      <td>1.46</td>\n",
              "      <td>2.52</td>\n",
              "      <td>1.33</td>\n",
              "      <td>5.12</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.23</td>\n",
              "      <td>1.70</td>\n",
              "      <td>2.52</td>\n",
              "      <td>...</td>\n",
              "      <td>1.17</td>\n",
              "      <td>0.13</td>\n",
              "      <td>1.95</td>\n",
              "      <td>0.90</td>\n",
              "      <td>1.46</td>\n",
              "      <td>4.39</td>\n",
              "      <td>3.13</td>\n",
              "      <td>0.25</td>\n",
              "      <td>5.61</td>\n",
              "      <td>3.74</td>\n",
              "      <td>4.28</td>\n",
              "      <td>3.29</td>\n",
              "      <td>2.15</td>\n",
              "      <td>5.01</td>\n",
              "      <td>0.73</td>\n",
              "      <td>1.15</td>\n",
              "      <td>0.25</td>\n",
              "      <td>3.17</td>\n",
              "      <td>5.93</td>\n",
              "      <td>4.28</td>\n",
              "      <td>4.02</td>\n",
              "      <td>0.93</td>\n",
              "      <td>1.70</td>\n",
              "      <td>5.12</td>\n",
              "      <td>2.76</td>\n",
              "      <td>3.18</td>\n",
              "      <td>5.24</td>\n",
              "      <td>0.68</td>\n",
              "      <td>1.09</td>\n",
              "      <td>2.68</td>\n",
              "      <td>2.37</td>\n",
              "      <td>1.21</td>\n",
              "      <td>5.85</td>\n",
              "      <td>2.03</td>\n",
              "      <td>0.74</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.08</td>\n",
              "      <td>1.84</td>\n",
              "      <td>2.57</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>30.10</td>\n",
              "      <td>11.85</td>\n",
              "      <td>0.48</td>\n",
              "      <td>8.10</td>\n",
              "      <td>0.11</td>\n",
              "      <td>9.39</td>\n",
              "      <td>1.05</td>\n",
              "      <td>5.75</td>\n",
              "      <td>1.96</td>\n",
              "      <td>1.51</td>\n",
              "      <td>0.74</td>\n",
              "      <td>5.24</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.01</td>\n",
              "      <td>4.75</td>\n",
              "      <td>2.27</td>\n",
              "      <td>0.36</td>\n",
              "      <td>5.00</td>\n",
              "      <td>1.39</td>\n",
              "      <td>4.75</td>\n",
              "      <td>4.02</td>\n",
              "      <td>2.03</td>\n",
              "      <td>1.60</td>\n",
              "      <td>2.80</td>\n",
              "      <td>1.27</td>\n",
              "      <td>0.25</td>\n",
              "      <td>6.83</td>\n",
              "      <td>1.78</td>\n",
              "      <td>1.82</td>\n",
              "      <td>8.42</td>\n",
              "      <td>0.78</td>\n",
              "      <td>3.53</td>\n",
              "      <td>6.83</td>\n",
              "      <td>2.03</td>\n",
              "      <td>4.04</td>\n",
              "      <td>3.29</td>\n",
              "      <td>1.66</td>\n",
              "      <td>3.18</td>\n",
              "      <td>4.14</td>\n",
              "      <td>1.15</td>\n",
              "      <td>...</td>\n",
              "      <td>2.64</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1.82</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.97</td>\n",
              "      <td>3.53</td>\n",
              "      <td>3.83</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.27</td>\n",
              "      <td>2.88</td>\n",
              "      <td>2.08</td>\n",
              "      <td>3.53</td>\n",
              "      <td>0.29</td>\n",
              "      <td>3.18</td>\n",
              "      <td>5.24</td>\n",
              "      <td>8.13</td>\n",
              "      <td>1.11</td>\n",
              "      <td>2.68</td>\n",
              "      <td>2.03</td>\n",
              "      <td>1.82</td>\n",
              "      <td>5.61</td>\n",
              "      <td>4.59</td>\n",
              "      <td>1.11</td>\n",
              "      <td>2.07</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.86</td>\n",
              "      <td>6.22</td>\n",
              "      <td>2.52</td>\n",
              "      <td>1.23</td>\n",
              "      <td>5.00</td>\n",
              "      <td>1.02</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.75</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.24</td>\n",
              "      <td>1.54</td>\n",
              "      <td>2.82</td>\n",
              "      <td>3.41</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 202 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       1      2      3      4      5  ...   198   199   200   201  Label\n",
              "0   5.93   0.60  10.00   4.93   5.24  ...  0.85  0.90  0.72  4.63      1\n",
              "1  18.97  14.29   4.77  21.07  10.63  ...  0.50  1.78  1.60  3.90      1\n",
              "2   9.23  12.20  15.14   3.47   7.68  ...  3.65  0.41  3.77  6.46      1\n",
              "3   9.69   8.17  11.10  13.62   8.55  ...  5.73  5.08  1.84  2.57      1\n",
              "4  30.10  11.85   0.48   8.10   0.11  ...  0.24  1.54  2.82  3.41      1\n",
              "\n",
              "[5 rows x 202 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 411
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlfGrk_9N-wf"
      },
      "source": [
        "The nominal task for this dataset is to predict the age from the other measurements, so separate the features and labels for training:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udOnDJOxNi7p"
      },
      "source": [
        "abalone_features = abalone_train.copy()\n",
        "abalone_labels = abalone_features.pop('Label')\n"
      ],
      "execution_count": 412,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seK9n71-UBfT"
      },
      "source": [
        "For this dataset you will treat all features identically. Pack the features into a single NumPy array.:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp3N5McbUMwb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67a1e826-4a29-417d-e11a-1e77985155c6"
      },
      "source": [
        "abalone_features = np.array(abalone_features)\n",
        "\n",
        "abalone_features\n"
      ],
      "execution_count": 413,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 5.93,  0.6 , 10.  , ...,  0.9 ,  0.72,  4.63],\n",
              "       [18.97, 14.29,  4.77, ...,  1.78,  1.6 ,  3.9 ],\n",
              "       [ 9.23, 12.2 , 15.14, ...,  0.41,  3.77,  6.46],\n",
              "       ...,\n",
              "       [11.42,  2.89, 50.07, ...,  0.66,  1.42,  1.57],\n",
              "       [23.87, 16.64, 27.12, ...,  2.49,  1.79,  1.85],\n",
              "       [ 3.24,  1.42,  5.88, ...,  0.78,  2.03,  0.38]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 413
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C1yFOxLOdxh"
      },
      "source": [
        "Next make a regression model predict the age. Since there is only a single input tensor, a `keras.Sequential` model is sufficient here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8zzNrZqOmfB"
      },
      "source": [
        "abalone_model = tf.keras.Sequential([\n",
        "  layers.Dense(67, activation='relu'),\n",
        "  layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "abalone_model.compile(loss = tf.losses.MeanSquaredError(),\n",
        "                      optimizer = tf.optimizers.Adam(),\n",
        "                      metrics = ['accuracy'])"
      ],
      "execution_count": 414,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6IWeP78O2wE"
      },
      "source": [
        "To train that model, pass the features and labels to `Model.fit`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZdpCD92SN3Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf4bfea0-5c6a-4552-f075-1c5267fcc6d6"
      },
      "source": [
        "abalone_model.fit(abalone_features, abalone_labels, epochs=100)"
      ],
      "execution_count": 415,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.3952 - accuracy: 0.1594\n",
            "Epoch 2/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.3958 - accuracy: 0.2580\n",
            "Epoch 3/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.3745 - accuracy: 0.2754\n",
            "Epoch 4/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.3565 - accuracy: 0.2464\n",
            "Epoch 5/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.3387 - accuracy: 0.2899\n",
            "Epoch 6/100\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3212 - accuracy: 0.2667\n",
            "Epoch 7/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.3067 - accuracy: 0.2435\n",
            "Epoch 8/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2979 - accuracy: 0.2493\n",
            "Epoch 9/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2854 - accuracy: 0.2406\n",
            "Epoch 10/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2788 - accuracy: 0.2551\n",
            "Epoch 11/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2735 - accuracy: 0.2667\n",
            "Epoch 12/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2694 - accuracy: 0.2493\n",
            "Epoch 13/100\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2663 - accuracy: 0.2464\n",
            "Epoch 14/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2623 - accuracy: 0.2145\n",
            "Epoch 15/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2602 - accuracy: 0.2435\n",
            "Epoch 16/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2580 - accuracy: 0.2551\n",
            "Epoch 17/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2558 - accuracy: 0.2493\n",
            "Epoch 18/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2543 - accuracy: 0.2116\n",
            "Epoch 19/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2527 - accuracy: 0.2638\n",
            "Epoch 20/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2517 - accuracy: 0.2348\n",
            "Epoch 21/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2509 - accuracy: 0.2493\n",
            "Epoch 22/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2493 - accuracy: 0.2522\n",
            "Epoch 23/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2486 - accuracy: 0.2493\n",
            "Epoch 24/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2478 - accuracy: 0.2406\n",
            "Epoch 25/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2470 - accuracy: 0.2522\n",
            "Epoch 26/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2468 - accuracy: 0.2203\n",
            "Epoch 27/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2460 - accuracy: 0.2551\n",
            "Epoch 28/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2458 - accuracy: 0.2551\n",
            "Epoch 29/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2456 - accuracy: 0.2609\n",
            "Epoch 30/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2458 - accuracy: 0.2116\n",
            "Epoch 31/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2454 - accuracy: 0.2928\n",
            "Epoch 32/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2453 - accuracy: 0.2203\n",
            "Epoch 33/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2449 - accuracy: 0.2841\n",
            "Epoch 34/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2448 - accuracy: 0.2290\n",
            "Epoch 35/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2445 - accuracy: 0.2580\n",
            "Epoch 36/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2442 - accuracy: 0.2145\n",
            "Epoch 37/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2443 - accuracy: 0.2899\n",
            "Epoch 38/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2446 - accuracy: 0.2377\n",
            "Epoch 39/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2443 - accuracy: 0.2290\n",
            "Epoch 40/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2441 - accuracy: 0.2783\n",
            "Epoch 41/100\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2436 - accuracy: 0.2580\n",
            "Epoch 42/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2435 - accuracy: 0.2667\n",
            "Epoch 43/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2434 - accuracy: 0.2348\n",
            "Epoch 44/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2436 - accuracy: 0.2348\n",
            "Epoch 45/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2432 - accuracy: 0.2261\n",
            "Epoch 46/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2434 - accuracy: 0.2290\n",
            "Epoch 47/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2431 - accuracy: 0.2812\n",
            "Epoch 48/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2432 - accuracy: 0.1623\n",
            "Epoch 49/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2426 - accuracy: 0.2783\n",
            "Epoch 50/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2422 - accuracy: 0.2232\n",
            "Epoch 51/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2420 - accuracy: 0.2435\n",
            "Epoch 52/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2421 - accuracy: 0.2348\n",
            "Epoch 53/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2421 - accuracy: 0.2029\n",
            "Epoch 54/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2421 - accuracy: 0.2348\n",
            "Epoch 55/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2420 - accuracy: 0.2348\n",
            "Epoch 56/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2421 - accuracy: 0.1971\n",
            "Epoch 57/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2418 - accuracy: 0.2145\n",
            "Epoch 58/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2423 - accuracy: 0.2493\n",
            "Epoch 59/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2424 - accuracy: 0.1971\n",
            "Epoch 60/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2420 - accuracy: 0.2261\n",
            "Epoch 61/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2415 - accuracy: 0.2087\n",
            "Epoch 62/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2416 - accuracy: 0.2406\n",
            "Epoch 63/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2415 - accuracy: 0.1652\n",
            "Epoch 64/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2415 - accuracy: 0.2261\n",
            "Epoch 65/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2417 - accuracy: 0.2551\n",
            "Epoch 66/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2418 - accuracy: 0.1971\n",
            "Epoch 67/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2416 - accuracy: 0.2145\n",
            "Epoch 68/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2417 - accuracy: 0.2029\n",
            "Epoch 69/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2417 - accuracy: 0.2667\n",
            "Epoch 70/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2415 - accuracy: 0.1739\n",
            "Epoch 71/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2413 - accuracy: 0.2435\n",
            "Epoch 72/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2413 - accuracy: 0.2029\n",
            "Epoch 73/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2412 - accuracy: 0.2464\n",
            "Epoch 74/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2413 - accuracy: 0.2116\n",
            "Epoch 75/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2411 - accuracy: 0.2000\n",
            "Epoch 76/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2412 - accuracy: 0.2261\n",
            "Epoch 77/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2412 - accuracy: 0.2029\n",
            "Epoch 78/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2410 - accuracy: 0.2348\n",
            "Epoch 79/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2409 - accuracy: 0.2029\n",
            "Epoch 80/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2410 - accuracy: 0.1884\n",
            "Epoch 81/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2410 - accuracy: 0.1942\n",
            "Epoch 82/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2410 - accuracy: 0.2174\n",
            "Epoch 83/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2410 - accuracy: 0.2029\n",
            "Epoch 84/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2411 - accuracy: 0.2551\n",
            "Epoch 85/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2411 - accuracy: 0.1884\n",
            "Epoch 86/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2410 - accuracy: 0.2464\n",
            "Epoch 87/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2409 - accuracy: 0.2377\n",
            "Epoch 88/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2409 - accuracy: 0.1855\n",
            "Epoch 89/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2408 - accuracy: 0.2377\n",
            "Epoch 90/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2408 - accuracy: 0.1826\n",
            "Epoch 91/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2409 - accuracy: 0.2232\n",
            "Epoch 92/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2409 - accuracy: 0.1797\n",
            "Epoch 93/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2409 - accuracy: 0.1797\n",
            "Epoch 94/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2409 - accuracy: 0.1971\n",
            "Epoch 95/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2408 - accuracy: 0.1768\n",
            "Epoch 96/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2409 - accuracy: 0.2348\n",
            "Epoch 97/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2408 - accuracy: 0.1710\n",
            "Epoch 98/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2409 - accuracy: 0.2232\n",
            "Epoch 99/100\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2409 - accuracy: 0.1971\n",
            "Epoch 100/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2410 - accuracy: 0.2580\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f858913d090>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 415
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GapLOj1OOTQH"
      },
      "source": [
        "You have just seen the most basic way to train a model using CSV data. Next, you will learn how to apply preprocessing to normalize numeric columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B87Rd1SOUv02"
      },
      "source": [
        "## Basic preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCrB2Jd-U0Vt"
      },
      "source": [
        "It's good practice to normalize the inputs to your model. The `experimental.preprocessing` layers provide a convenient way to build this normalization into your model. \n",
        "\n",
        "The layer will precompute the mean and variance of each column, and use these to normalize the data.\n",
        "\n",
        "First you create the layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2WQpDU5VRk7"
      },
      "source": [
        "normalize = preprocessing.Normalization()"
      ],
      "execution_count": 416,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGgEZE-7Vpt6"
      },
      "source": [
        "Then you use the `Normalization.adapt()` method to adapt the normalization layer to your data.\n",
        "\n",
        "Note: Only use your training data to `.adapt()` preprocessing layers. Do not use your validation or test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WgOPIiOVpLg"
      },
      "source": [
        "normalize.adapt(abalone_features)"
      ],
      "execution_count": 417,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE6vh0byV7cE"
      },
      "source": [
        "Then use the normalization layer in your model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quPcZ9dTWA9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81cebafd-d6fa-4017-e96b-90cf4396f6c1"
      },
      "source": [
        "norm_abalone_model = tf.keras.Sequential([\n",
        "  normalize,\n",
        "  layers.Dense(67, activation='relu'),\n",
        "  layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "norm_abalone_model.compile(loss = tf.losses.MeanSquaredError(),\n",
        "                           optimizer = tf.optimizers.Adam(),\n",
        "                           metrics=['accuracy'])\n",
        "\n",
        "norm_abalone_model.fit(abalone_features, abalone_labels, epochs=100)\n"
      ],
      "execution_count": 418,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.3113 - accuracy: 0.1797\n",
            "Epoch 2/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2817 - accuracy: 0.2087\n",
            "Epoch 3/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2663 - accuracy: 0.2435\n",
            "Epoch 4/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2573 - accuracy: 0.2464\n",
            "Epoch 5/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2521 - accuracy: 0.2493\n",
            "Epoch 6/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2490 - accuracy: 0.2348\n",
            "Epoch 7/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2467 - accuracy: 0.2348\n",
            "Epoch 8/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2449 - accuracy: 0.2261\n",
            "Epoch 9/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2440 - accuracy: 0.2203\n",
            "Epoch 10/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2431 - accuracy: 0.2377\n",
            "Epoch 11/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2424 - accuracy: 0.2319\n",
            "Epoch 12/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2420 - accuracy: 0.2290\n",
            "Epoch 13/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2417 - accuracy: 0.2551\n",
            "Epoch 14/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2415 - accuracy: 0.2435\n",
            "Epoch 15/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2413 - accuracy: 0.2232\n",
            "Epoch 16/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2412 - accuracy: 0.2493\n",
            "Epoch 17/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2412 - accuracy: 0.2464\n",
            "Epoch 18/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2411 - accuracy: 0.2319\n",
            "Epoch 19/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2409 - accuracy: 0.2522\n",
            "Epoch 20/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2409 - accuracy: 0.2609\n",
            "Epoch 21/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2409 - accuracy: 0.2435\n",
            "Epoch 22/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2408 - accuracy: 0.2841\n",
            "Epoch 23/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2408 - accuracy: 0.2290\n",
            "Epoch 24/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2408 - accuracy: 0.3246\n",
            "Epoch 25/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2408 - accuracy: 0.2377\n",
            "Epoch 26/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2409 - accuracy: 0.2957\n",
            "Epoch 27/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2408 - accuracy: 0.2116\n",
            "Epoch 28/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2408 - accuracy: 0.3333\n",
            "Epoch 29/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2409 - accuracy: 0.2319\n",
            "Epoch 30/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2409 - accuracy: 0.3362\n",
            "Epoch 31/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2409 - accuracy: 0.2203\n",
            "Epoch 32/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2409 - accuracy: 0.3246\n",
            "Epoch 33/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2410 - accuracy: 0.1826\n",
            "Epoch 34/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2410 - accuracy: 0.3536\n",
            "Epoch 35/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2409 - accuracy: 0.1913\n",
            "Epoch 36/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2410 - accuracy: 0.3043\n",
            "Epoch 37/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2410 - accuracy: 0.0638\n",
            "Epoch 38/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2410 - accuracy: 0.2957\n",
            "Epoch 39/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2410 - accuracy: 0.1971\n",
            "Epoch 40/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2411 - accuracy: 0.2551\n",
            "Epoch 41/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2412 - accuracy: 0.1942\n",
            "Epoch 42/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2414 - accuracy: 0.2899\n",
            "Epoch 43/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2412 - accuracy: 0.1884\n",
            "Epoch 44/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2413 - accuracy: 0.2551\n",
            "Epoch 45/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2411 - accuracy: 0.2145\n",
            "Epoch 46/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2415 - accuracy: 0.2493\n",
            "Epoch 47/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2413 - accuracy: 0.1188\n",
            "Epoch 48/100\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2411 - accuracy: 0.3072\n",
            "Epoch 49/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2410 - accuracy: 0.0986\n",
            "Epoch 50/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2410 - accuracy: 0.2899\n",
            "Epoch 51/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2410 - accuracy: 0.1478\n",
            "Epoch 52/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2411 - accuracy: 0.2377\n",
            "Epoch 53/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2411 - accuracy: 0.1710\n",
            "Epoch 54/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2413 - accuracy: 0.2348\n",
            "Epoch 55/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2411 - accuracy: 0.1449\n",
            "Epoch 56/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2411 - accuracy: 0.2928\n",
            "Epoch 57/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2410 - accuracy: 0.1159\n",
            "Epoch 58/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2410 - accuracy: 0.2899\n",
            "Epoch 59/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2410 - accuracy: 0.1971\n",
            "Epoch 60/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2410 - accuracy: 0.2696\n",
            "Epoch 61/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2410 - accuracy: 0.2029\n",
            "Epoch 62/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2410 - accuracy: 0.2174\n",
            "Epoch 63/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2412 - accuracy: 0.1797\n",
            "Epoch 64/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2410 - accuracy: 0.2377\n",
            "Epoch 65/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2412 - accuracy: 0.1797\n",
            "Epoch 66/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2413 - accuracy: 0.2290\n",
            "Epoch 67/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2416 - accuracy: 0.1652\n",
            "Epoch 68/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2414 - accuracy: 0.2638\n",
            "Epoch 69/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2414 - accuracy: 0.1478\n",
            "Epoch 70/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2412 - accuracy: 0.2754\n",
            "Epoch 71/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2413 - accuracy: 0.1565\n",
            "Epoch 72/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2413 - accuracy: 0.2290\n",
            "Epoch 73/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2417 - accuracy: 0.2406\n",
            "Epoch 74/100\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2418 - accuracy: 0.1449\n",
            "Epoch 75/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2415 - accuracy: 0.3159\n",
            "Epoch 76/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2413 - accuracy: 0.2203\n",
            "Epoch 77/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2410 - accuracy: 0.2174\n",
            "Epoch 78/100\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2411 - accuracy: 0.2232\n",
            "Epoch 79/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2411 - accuracy: 0.2203\n",
            "Epoch 80/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2410 - accuracy: 0.2522\n",
            "Epoch 81/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2408 - accuracy: 0.2435\n",
            "Epoch 82/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2408 - accuracy: 0.1710\n",
            "Epoch 83/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2407 - accuracy: 0.2696\n",
            "Epoch 84/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2408 - accuracy: 0.1913\n",
            "Epoch 85/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2408 - accuracy: 0.2319\n",
            "Epoch 86/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2409 - accuracy: 0.1449\n",
            "Epoch 87/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2410 - accuracy: 0.1797\n",
            "Epoch 88/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2410 - accuracy: 0.1942\n",
            "Epoch 89/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2409 - accuracy: 0.2435\n",
            "Epoch 90/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2410 - accuracy: 0.1594\n",
            "Epoch 91/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2410 - accuracy: 0.2290\n",
            "Epoch 92/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2409 - accuracy: 0.1159\n",
            "Epoch 93/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2410 - accuracy: 0.2638\n",
            "Epoch 94/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2410 - accuracy: 0.1623\n",
            "Epoch 95/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2412 - accuracy: 0.1942\n",
            "Epoch 96/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2411 - accuracy: 0.2029\n",
            "Epoch 97/100\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 3.2413 - accuracy: 0.1652\n",
            "Epoch 98/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2413 - accuracy: 0.2319\n",
            "Epoch 99/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2410 - accuracy: 0.1768\n",
            "Epoch 100/100\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 3.2410 - accuracy: 0.2348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8587f87110>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 418
        }
      ]
    }
  ]
}